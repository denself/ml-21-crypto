{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "local = True\n",
    "if local:\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Spark session & context\n",
    "    builder = SparkSession.builder.master('local[*]')\n",
    "    builder.config('spark.jars.packages', 'org.apache.spark:spark-avro_2.12:3.1.1')\n",
    "    spark = builder.getOrCreate()\n",
    "\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_prefix = 'work/data/' if local else 'gs://exported-data-ucu-2/'\n",
    "# Convert Avro to Parquet\n",
    "spark.read \\\n",
    "    .format(\"avro\").load(f\"{path_prefix}wallets/*\") \\\n",
    "    .write.parquet(f\"{path_prefix}wallets_p/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.read \\\n",
    "    .format(\"avro\").load(f\"{path_prefix}sent_q/*\") \\\n",
    "    .write.parquet(f\"{path_prefix}sent_q_p/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.read \\\n",
    "    .format(\"avro\").load(f\"{path_prefix}received_q/*\") \\\n",
    "    .write.parquet(f\"{path_prefix}received_q_p/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load wallets data\n",
    "wallets_raw = spark.read.format(\"parquet\").load(\"gs://exported-data-ucu-2/wallets-parquet/*\")\n",
    "# wallets_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    'balance', 'sent_trx_number', 'received_trx_number', 'sent_total', 'sent_min', 'sent_avg',\n",
    "    'sent_max', 'received_total', 'received_min', 'received_avg', 'received_max', 'min_inputs',\n",
    "    'avg_inputs', 'max_inputs', 'min_outputs', 'avg_outputs', 'max_outputs',\n",
    "    'age_days', 'sent_range_days', 'received_range_days', 'sent_inactive_days', 'received_inactive_days',\n",
    "    'send_freq', 'received_freq', 'has_coinbase'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 0) Vectorization\n",
    "#################\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "wallets_vector = assembler.transform(wallets_raw).select('address', 'features')\n",
    "# wallets_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 1) SCALING / NORMALIZATION\n",
    "#################\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaledFeatures\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(wallets_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerModel.save(f\"{path_prefix}models/scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize each feature to have unit standard deviation.\n",
    "wallets_scaled = scalerModel.transform(wallets_vector).select(['address', 'scaledFeatures'])\n",
    "# wallets_scaled.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 2) RUN PCA ON ALL 24 FEATURES, FIND OUT HOW MANY DIMENSIONS WE NEED\n",
    "#################\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "if local:\n",
    "    import numpy as np\n",
    "    pca = PCA(k=len(features), inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "    pca_model = pca.fit(wallets_scaled)\n",
    "    print(np.cumsum(pca_model.explainedVariance.toArray()))\n",
    "    i = np.searchsorted(np.cumsum(pca_model.explainedVariance.toArray()), 0.9, side='right')\n",
    "else:\n",
    "    i = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 3) REDUCTION OF FEATURES, KEEP AS MANY AS NEEDED\n",
    "#################\n",
    "\n",
    "pca = PCA(k=i, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(wallets_scaled)\n",
    "wallets_pcs = pca_model.transform(wallets_scaled).select(['address', \"pcaFeatures\"])\n",
    "# wallets_pcs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model.save(f\"{path_prefix}models/pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 4) CLUSTERING\n",
    "#################\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans(featuresCol='pcaFeatures').setK(5).setSeed(42)\n",
    "clustering_model = kmeans.fit(wallets_pcs)\n",
    "\n",
    "# Make predictions\n",
    "wallets_clustered = clustering_model.transform(wallets_pcs).select('address', 'prediction')\n",
    "# wallets_clustered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_model.save(f\"{path_prefix}/models/kmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 5) QUERY SENT AMOUNTS OF BTC FROM EACH WALLET PER HOUR\n",
    "#################\n",
    "sent = spark.read.format(\"parquet\")\\\n",
    "    .load(f\"{path_prefix}/sent_q_p/*\")\\\n",
    "    .withColumnRenamed(\"sum\", \"sum_sent\")\n",
    "# sent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 6) QUERY RECEIVED AMOUNTS OF BTC TO EACH WALLET PER HOUR\n",
    "#################\n",
    "received = spark.read.format(\"parquet\")\\\n",
    "    .load(f\"{path_prefix}/received_q_p/*\")\\\n",
    "    .withColumnRenamed(\"sum\", \"sum_received\")\n",
    "# received.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 7) JOIN AND GROUP ALL DATA\n",
    "#################\n",
    "\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "cond = ['date_hour', 'address']\n",
    "sent_received = sent.join(received, cond, 'outer')\n",
    "# sent_received.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_data = sent_received.join(wallets_clustered, 'address', 'left')\n",
    "# all_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clustered_transactions = all_data.groupby(all_data.date_hour, all_data.prediction)\\\n",
    "    .pivot(\"prediction\")\\\n",
    "    .sum(\"sum_sent\", \"sum_received\")\\\n",
    "    .sort(asc(\"date_hour\"))\n",
    "# clustered_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## 8) OUTPUT CALCULATION RESULTS\n",
    "#################\n",
    "\n",
    "clustered_transactions \\\n",
    "    .withColumnRenamed(\"null_sum(sum_sent)\",\"null_sum_sent\")\\\n",
    "    .withColumnRenamed(\"null_sum(sum_received)\",\"null_sum_received\")\\\n",
    "    .withColumnRenamed(\"0_sum(sum_sent)\",\"0_sum_sent\")\\\n",
    "    .withColumnRenamed(\"0_sum(sum_received)\",\"0_sum_received\")\\\n",
    "    .withColumnRenamed(\"1_sum(sum_sent)\",\"1_sum_sent\")\\\n",
    "    .withColumnRenamed(\"1_sum(sum_received)\",\"1_sum_received\")\\\n",
    "    .withColumnRenamed(\"2_sum(sum_sent)\",\"2_sum_sent\")\\\n",
    "    .withColumnRenamed(\"2_sum(sum_received)\",\"2_sum_received\")\\\n",
    "    .withColumnRenamed(\"3_sum(sum_sent)\",\"3_sum_sent\")\\\n",
    "    .withColumnRenamed(\"3_sum(sum_received)\",\"3_sum_received\")\\\n",
    "    .withColumnRenamed(\"4_sum(sum_sent)\",\"4_sum_sent\")\\\n",
    "    .withColumnRenamed(\"4_sum(sum_received)\",\"4_sum_received\")\\\n",
    "    .write.parquet(f\"{path_prefix}/out/clustered_transactions_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_transactions_up = spark.read.format(\"parquet\")\\\n",
    "    .load(f\"{path_prefix}/out/clustered_transactions_p/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clustered_transactions_up.groupby(clustered_transactions_up.date_hour).sum() \\\n",
    "    .drop('sum(prediction)') \\\n",
    "    .withColumnRenamed(\"sum(null_sum_sent)\", \"null_sum_sent\") \\\n",
    "    .withColumnRenamed(\"sum(null_sum_received)\", \"null_sum_received\") \\\n",
    "    .withColumnRenamed(\"sum(0_sum_sent)\", \"0_sum_sent\") \\\n",
    "    .withColumnRenamed(\"sum(0_sum_received)\", \"0_sum_received\") \\\n",
    "    .withColumnRenamed(\"sum(1_sum_sent)\", \"1_sum_sent\") \\\n",
    "    .withColumnRenamed(\"sum(1_sum_received)\", \"1_sum_received\") \\\n",
    "    .withColumnRenamed(\"sum(2_sum_sent)\", \"2_sum_sent\") \\\n",
    "    .withColumnRenamed(\"sum(2_sum_received)\", \"2_sum_received\") \\\n",
    "    .withColumnRenamed(\"sum(3_sum_sent)\", \"3_sum_sent\") \\\n",
    "    .withColumnRenamed(\"sum(3_sum_received)\", \"3_sum_received\") \\\n",
    "    .withColumnRenamed(\"sum(4_sum_sent)\", \"4_sum_sent\") \\\n",
    "    .withColumnRenamed(\"sum(4_sum_received)\", \"4_sum_received\") \\\n",
    "    .write.parquet(f\"{path_prefix}/out_p_g\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}